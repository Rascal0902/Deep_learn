import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import argparse
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# 하이퍼파라미터
batch_size = 100
learning_rate = 0.0002
num_epoch = 10

# 데이터 증강 및 전처리
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ToTensor()
])

mnist_train = datasets.MNIST(root="../Deep_learn/Data/", train=True, transform=transform, download=True)
mnist_test = datasets.MNIST(root="../Deep_learn/Data/", train=False, transform=transforms.ToTensor(), download=True)

train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)
test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True)

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()

        self.layer = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5),
            nn.ReLU(),
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(64 * 3 * 3, 100),
            nn.ReLU(),
            nn.Dropout(p=0.5),
            nn.Linear(100, 10)
        )

    def forward(self, x):
        out = self.layer(x)
        out = out.view(out.size(0), -1)
        out = self.fc_layer(out)
        return out

def save_checkpoint(model, optimizer, epoch, loss, filename='checkpoint.pth'):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss
    }
    torch.save(checkpoint, filename)
    print(f'Checkpoint saved to {filename}')

def load_checkpoint(model, optimizer, filename='checkpoint.pth'):
    checkpoint = torch.load(filename)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    print(f'Checkpoint loaded from {filename}')
    return start_epoch, loss

# Argument parser 설정
parser = argparse.ArgumentParser(description='PyTorch MNIST Training')
parser.add_argument('--resume', type=str, default=None, help='path to checkpoint to resume training')
args = parser.parse_args()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = CNN().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

start_epoch = 0
if args.resume:
    start_epoch, _ = load_checkpoint(model, optimizer, args.resume)

loss_func = nn.CrossEntropyLoss()
loss_arr = []

for epoch in range(start_epoch, num_epoch):
    model.train()
    for j, [image, label] in enumerate(train_loader):
        x = image.to(device)
        y = label.to(device)

        optimizer.zero_grad()
        output = model(x)
        loss = loss_func(output, y)
        loss.backward()
        optimizer.step()

        if j % 1000 == 0:
            print(f"Epoch [{epoch+1}/{num_epoch}], Step [{j}/{len(train_loader)}], Loss: {loss.item()}")
            loss_arr.append(loss.cpu().detach().numpy())

    if (epoch + 1) % 1 == 0:
        save_checkpoint(model, optimizer, epoch + 1, loss.item())

    scheduler.step()

# 모델 평가 및 혼동 행렬 시각화
correct = 0
total = 0
y_true = []
y_pred = []

model.eval()
with torch.no_grad():
    for image, label in test_loader:
        x = image.to(device)
        y = label.to(device)

        output = model(x)
        _, output_index = torch.max(output, 1)

        total += label.size(0)
        correct += (output_index == y).sum().float()

        y_true.extend(y.cpu().numpy())
        y_pred.extend(output_index.cpu().numpy())

    print("Accuracy of Test Data: {}%".format(100 * correct / total))

# 혼동 행렬 시각화
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()